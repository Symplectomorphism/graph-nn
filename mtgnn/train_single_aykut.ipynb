{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from net import gtnet\n",
    "import importlib\n",
    "\n",
    "from util import *\n",
    "from trainer import Optim\n",
    "from generate_index_data import get_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  10 of 10 completed\n",
      "/home/acs/repos/research/graph-nn/mtgnn/generate_index_data.py:23: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  nse30['Open'].iloc[i] = locale.atof(nse30['Open'].iloc[i])\n",
      "/home/acs/repos/research/graph-nn/mtgnn/generate_index_data.py:32: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  nse30 = nse30.reindex(idx, fill_value=np.nan).ffill()\n"
     ]
    }
   ],
   "source": [
    "data = get_stock_data(dt.datetime(2005, 1, 3), dt.datetime(2024, 8, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, X, Y, model, evaluateL2, evaluateL1, batch_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_l1 = 0\n",
    "    n_samples = 0\n",
    "    predict = None\n",
    "    test = None\n",
    "\n",
    "    for X, Y in data.get_batches(X, Y, batch_size, False):\n",
    "        X = torch.unsqueeze(X,dim=1)\n",
    "        X = X.transpose(2,3)\n",
    "        with torch.no_grad():\n",
    "            output = model(X)\n",
    "        output = torch.squeeze(output)\n",
    "        if len(output.shape)==1:\n",
    "            output = output.unsqueeze(dim=0)\n",
    "        if predict is None:\n",
    "            predict = output\n",
    "            test = Y\n",
    "        else:\n",
    "            predict = torch.cat((predict, output))\n",
    "            test = torch.cat((test, Y))\n",
    "\n",
    "        scale = data.scale.expand(output.size(0), data.m)\n",
    "        total_loss += evaluateL2(output * scale, Y * scale).item()\n",
    "        total_loss_l1 += evaluateL1(output * scale, Y * scale).item()\n",
    "        n_samples += (output.size(0) * data.m)\n",
    "\n",
    "    rse = math.sqrt(total_loss / n_samples) / data.rse\n",
    "    rae = (total_loss_l1 / n_samples) / data.rae\n",
    "\n",
    "    predict = predict.data.cpu().numpy()\n",
    "    Ytest = test.data.cpu().numpy()\n",
    "    sigma_p = (predict).std(axis=0)\n",
    "    sigma_g = (Ytest).std(axis=0)\n",
    "    mean_p = predict.mean(axis=0)\n",
    "    mean_g = Ytest.mean(axis=0)\n",
    "    index = (sigma_g != 0)\n",
    "    correlation = ((predict - mean_p) * (Ytest - mean_g)).mean(axis=0) / (sigma_p * sigma_g)\n",
    "    correlation = (correlation[index]).mean()\n",
    "    return rse, rae, correlation\n",
    "\n",
    "\n",
    "def train(data, X, Y, model, criterion, optim, batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_samples = 0\n",
    "    iter = 0\n",
    "    for X, Y in data.get_batches(X, Y, batch_size, True):\n",
    "        model.zero_grad()\n",
    "        X = torch.unsqueeze(X,dim=1)\n",
    "        X = X.transpose(2,3)\n",
    "        if iter % args.step_size == 0:\n",
    "            perm = np.random.permutation(range(args.num_nodes))\n",
    "        num_sub = int(args.num_nodes / args.num_split)\n",
    "\n",
    "        for j in range(args.num_split):\n",
    "            if j != args.num_split - 1:\n",
    "                id = perm[j * num_sub:(j + 1) * num_sub]\n",
    "            else:\n",
    "                id = perm[j * num_sub:]\n",
    "            id = torch.tensor(id).to(device)\n",
    "            tx = X[:, :, id, :]\n",
    "            ty = Y[:, id]\n",
    "            output = model(tx,id)\n",
    "            output = torch.squeeze(output)\n",
    "            scale = data.scale.expand(output.size(0), data.m)\n",
    "            scale = scale[:,id]\n",
    "            loss = criterion(output * scale, ty * scale)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            n_samples += (output.size(0) * data.m)\n",
    "            grad_norm = optim.step()\n",
    "\n",
    "        if iter%100==0:\n",
    "            print('iter:{:3d} | loss: {:.3f}'.format(iter,loss.item()/(output.size(0) * data.m)))\n",
    "        iter += 1\n",
    "    return total_loss / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Time series forecasting')\n",
    "parser.add_argument('--data', type=str, default='./data/indices.csv',\n",
    "                    help='location of the data file')\n",
    "parser.add_argument('--log_interval', type=int, default=2000, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str, default='model/model-ind.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--optim', type=str, default='adam')\n",
    "parser.add_argument('--L1Loss', type=bool, default=True)\n",
    "parser.add_argument('--normalize', type=int, default=2)\n",
    "parser.add_argument('--device',type=str,default='cuda:0',help='')\n",
    "parser.add_argument('--gcn_true', type=bool, default=True, help='whether to add graph convolution layer')\n",
    "parser.add_argument('--buildA_true', type=bool, default=True, help='whether to construct adaptive adjacency matrix')\n",
    "parser.add_argument('--gcn_depth',type=int,default=2,help='graph convolution depth')\n",
    "# parser.add_argument('--num_nodes',type=int,default=137,help='number of nodes/variables')\n",
    "parser.add_argument('--num_nodes',type=int,default=11,help='number of nodes/variables')\n",
    "parser.add_argument('--dropout',type=float,default=0.3,help='dropout rate')\n",
    "# parser.add_argument('--subgraph_size',type=int,default=20,help='k')\n",
    "parser.add_argument('--subgraph_size',type=int,default=5,help='k')\n",
    "parser.add_argument('--node_dim',type=int,default=40,help='dim of nodes')\n",
    "parser.add_argument('--dilation_exponential',type=int,default=2,help='dilation exponential')\n",
    "parser.add_argument('--conv_channels',type=int,default=16,help='convolution channels')\n",
    "parser.add_argument('--residual_channels',type=int,default=16,help='residual channels')\n",
    "parser.add_argument('--skip_channels',type=int,default=32,help='skip channels')\n",
    "parser.add_argument('--end_channels',type=int,default=64,help='end channels')\n",
    "parser.add_argument('--in_dim',type=int,default=1,help='inputs dimension')\n",
    "parser.add_argument('--seq_in_len',type=int,default=24*7,help='input sequence length')\n",
    "parser.add_argument('--seq_out_len',type=int,default=1,help='output sequence length')\n",
    "parser.add_argument('--horizon', type=int, default=3)\n",
    "parser.add_argument('--layers',type=int,default=5,help='number of layers')\n",
    "\n",
    "# parser.add_argument('--batch_size',type=int,default=32,help='batch size')\n",
    "parser.add_argument('--batch_size',type=int,default=8,help='batch size')\n",
    "parser.add_argument('--lr',type=float,default=0.0001,help='learning rate')\n",
    "parser.add_argument('--weight_decay',type=float,default=0.00001,help='weight decay rate')\n",
    "\n",
    "parser.add_argument('--clip',type=int,default=5,help='clip')\n",
    "\n",
    "parser.add_argument('--propalpha',type=float,default=0.05,help='prop alpha')\n",
    "parser.add_argument('--tanhalpha',type=float,default=3,help='tanh alpha')\n",
    "\n",
    "# parser.add_argument('--epochs',type=int,default=1,help='')\n",
    "parser.add_argument('--epochs',type=int,default=30,help='')\n",
    "parser.add_argument('--num_split',type=int,default=1,help='number of splits for graphs')\n",
    "parser.add_argument('--step_size',type=int,default=100,help='step_size')\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "device = torch.device(args.device)\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data='./data/indices.csv', log_interval=2000, save='model/model-ind.pt', optim='adam', L1Loss=True, normalize=2, device='cuda:0', gcn_true=True, buildA_true=True, gcn_depth=2, num_nodes=11, dropout=0.3, subgraph_size=5, node_dim=40, dilation_exponential=2, conv_channels=16, residual_channels=16, skip_channels=32, end_channels=64, in_dim=1, seq_in_len=168, seq_out_len=1, horizon=3, layers=5, batch_size=8, lr=0.0001, weight_decay=1e-05, clip=5, propalpha=0.05, tanhalpha=3, epochs=30, num_split=1, step_size=100)\n",
      "The receptive field size is 187\n",
      "Number of model parameters is 337585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acs/miniforge3/envs/mtgnn/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "Data = DataLoaderS(args.data, 0.6, 0.2, device, args.horizon, args.seq_in_len, args.normalize)\n",
    "\n",
    "model = gtnet(args.gcn_true, args.buildA_true, args.gcn_depth, args.num_nodes,\n",
    "                device, dropout=args.dropout, subgraph_size=args.subgraph_size,\n",
    "                node_dim=args.node_dim, dilation_exponential=args.dilation_exponential,\n",
    "                conv_channels=args.conv_channels, residual_channels=args.residual_channels,\n",
    "                skip_channels=args.skip_channels, end_channels= args.end_channels,\n",
    "                seq_length=args.seq_in_len, in_dim=args.in_dim, out_dim=args.seq_out_len,\n",
    "                layers=args.layers, propalpha=args.propalpha, tanhalpha=args.tanhalpha, layer_norm_affline=False)\n",
    "model = model.to(device)\n",
    "\n",
    "print(args)\n",
    "print('The receptive field size is', model.receptive_field)\n",
    "nParams = sum([p.nelement() for p in model.parameters()])\n",
    "print('Number of model parameters is', nParams, flush=True)\n",
    "\n",
    "if args.L1Loss:\n",
    "    criterion = nn.L1Loss(size_average=False).to(device)\n",
    "else:\n",
    "    criterion = nn.MSELoss(size_average=False).to(device)\n",
    "evaluateL2 = nn.MSELoss(size_average=False).to(device)\n",
    "evaluateL1 = nn.L1Loss(size_average=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model):\n",
    "\n",
    "    best_val = 10000000\n",
    "    optim = Optim(\n",
    "        model.parameters(), args.optim, args.lr, args.clip, lr_decay=args.weight_decay\n",
    "    )\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    try:\n",
    "        print('begin training')\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            train_loss = train(Data, Data.train[0], Data.train[1], model, criterion, optim, args.batch_size)\n",
    "            val_loss, val_rae, val_corr = evaluate(Data, Data.valid[0], Data.valid[1], model, evaluateL2, evaluateL1,\n",
    "                                               args.batch_size)\n",
    "            print(\n",
    "                '| end of epoch {:3d} | time: {:5.2f}s | train_loss {:5.4f} | valid rse {:5.4f} | valid rae {:5.4f} | valid corr  {:5.4f}'.format(\n",
    "                    epoch, (time.time() - epoch_start_time), train_loss, val_loss, val_rae, val_corr), flush=True)\n",
    "            # print(torch.round(model.gc(model.idx), decimals=4))\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "\n",
    "            if val_loss < best_val:\n",
    "                with open(args.save, 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                best_val = val_loss\n",
    "            if epoch % 5 == 0:\n",
    "                test_acc, test_rae, test_corr = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1,\n",
    "                                                     args.batch_size)\n",
    "                print(\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr), flush=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        print('Exiting from training early')\n",
    "\n",
    "    # Load the best saved model.\n",
    "    with open(args.save, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "\n",
    "    vtest_acc, vtest_rae, vtest_corr = evaluate(Data, Data.valid[0], Data.valid[1], model, evaluateL2, evaluateL1,\n",
    "                                         args.batch_size)\n",
    "    test_acc, test_rae, test_corr = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1,\n",
    "                                         args.batch_size)\n",
    "    print(\"final test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n",
    "    return vtest_acc, vtest_rae, vtest_corr, test_acc, test_rae, test_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "iter:  0 | loss: 6.893\n",
      "iter:100 | loss: 0.325\n",
      "iter:200 | loss: 0.227\n",
      "iter:300 | loss: 0.211\n",
      "| end of epoch   1 | time:  4.34s | train_loss 0.4261 | valid rse 0.1601 | valid rae 0.1398 | valid corr  0.6924\n",
      "tensor([[0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.9992],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.9988, 0.0000, 0.0000, 0.9981, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
      "         0.0000, 1.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000],\n",
      "        [1.0000, 0.9982, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.9980, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000],\n",
      "        [0.0000, 0.9971, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.9991, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.9998, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.9997, 0.9993, 0.0000, 0.9998, 1.0000, 0.0000, 0.9996, 0.0000,\n",
      "         0.0000, 0.0000]], device='cuda:0', grad_fn=<RoundBackward1>)\n",
      "iter:  0 | loss: 0.261\n",
      "iter:100 | loss: 0.208\n",
      "iter:200 | loss: 0.171\n",
      "iter:300 | loss: 0.162\n",
      "| end of epoch   2 | time:  5.00s | train_loss 0.1969 | valid rse 0.1085 | valid rae 0.0942 | valid corr  0.7167\n",
      "tensor([[0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
      "         0.0000, 0.9996],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.9972, 0.0000, 0.0000, 0.9995, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
      "         0.0000, 1.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000],\n",
      "        [1.0000, 0.9987, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.9990, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000],\n",
      "        [0.0000, 0.9984, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.9988, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.9998, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.9999, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.9998, 0.9992, 0.0000, 0.9998, 1.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], device='cuda:0', grad_fn=<RoundBackward1>)\n",
      "iter:  0 | loss: 0.163\n",
      "iter:100 | loss: 0.139\n",
      "iter:200 | loss: 0.151\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_204476/3762187797.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test rse 0.2054 | test rae 0.2044 | test corr 0.3231\n",
      "begin training\n",
      "iter:  0 | loss: 0.170\n",
      "iter:100 | loss: 0.106\n",
      "iter:200 | loss: 0.140\n",
      "iter:300 | loss: 0.123\n",
      "| end of epoch   1 | time:  8.73s | train_loss 0.1312 | valid rse 0.1100 | valid rae 0.0874 | valid corr  0.7643\n",
      "tensor([[0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
      "         0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.9999, 0.0000, 0.0000, 0.9999, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
      "         0.0000, 1.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.9998, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.9998, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.9997, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], device='cuda:0', grad_fn=<RoundBackward1>)\n",
      "iter:  0 | loss: 0.121\n",
      "iter:100 | loss: 0.092\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "final test rse 0.1391 | test rae 0.1300 | test corr 0.4265\n",
      "begin training\n",
      "iter:  0 | loss: 0.108\n",
      "iter:100 | loss: 0.095\n",
      "iter:200 | loss: 0.124\n",
      "iter:300 | loss: 0.077\n",
      "| end of epoch   1 | time:  8.26s | train_loss 0.1082 | valid rse 0.1774 | valid rae 0.1882 | valid corr  0.7947\n",
      "tensor([[0.0000, 0.0000, 1.0000, 1.0000, 0.9999, 0.0000, 0.0000, 1.0000, 1.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
      "         0.0000, 1.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], device='cuda:0', grad_fn=<RoundBackward1>)\n",
      "iter:  0 | loss: 0.127\n"
     ]
    }
   ],
   "source": [
    "vacc = []\n",
    "vrae = []\n",
    "vcorr = []\n",
    "acc = []\n",
    "rae = []\n",
    "corr = []\n",
    "for i in range(10):\n",
    "    val_acc, val_rae, val_corr, test_acc, test_rae, test_corr = run(model)\n",
    "    vacc.append(val_acc)\n",
    "    vrae.append(val_rae)\n",
    "    vcorr.append(val_corr)\n",
    "    acc.append(test_acc)\n",
    "    rae.append(test_rae)\n",
    "    corr.append(test_corr)\n",
    "print('\\n\\n')\n",
    "print('10 runs average')\n",
    "print('\\n\\n')\n",
    "print(\"valid\\trse\\trae\\tcorr\")\n",
    "print(\"mean\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.mean(vacc), np.mean(vrae), np.mean(vcorr)))\n",
    "print(\"std\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.std(vacc), np.std(vrae), np.std(vcorr)))\n",
    "print('\\n\\n')\n",
    "print(\"test\\trse\\trae\\tcorr\")\n",
    "print(\"mean\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.mean(acc), np.mean(rae), np.mean(corr)))\n",
    "print(\"std\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.std(acc), np.std(rae), np.std(corr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
